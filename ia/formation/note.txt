###############################################################################
Mon Jul 13 11:00:02 CEST 2020

Recherche du cours
   https://openclassrooms.com/fr/courses/5801891-initiez-vous-au-deep-learning/5801898-decouvrez-le-neurone-formel
      8h
      certificat de réussite à la clef

1. Découvrez le neurone formel

   Le neurone biologique

      RNN est sont des modèles mathématiques du neurone biologique

      3 grandes entités
         corps cellulaire
            appelé péricaryon
         ensemble de dentrites (~7000)
            ce sont les capteurs du neurone
            transmettent l'influx nerveux de leur extrémité au péricaryon
               le flux est dit centripète
         axone

      cf Complete_neuron_cell_diagram_fr.svg.png

      Qd exitation du corps cellulaire dépasse un seuil, exitation se fait par !!modulation de fréquence!!

      Qd etremité de l'axone est en contact ac dentrite d'un autre neurone
         la zone de contact est un !!synapse!!
         permet transmition de donnée entre neurone
         c'est un réseau


   Le neurone formel

      Appelé aussi !!perceptron!!
                   !!neurone formel!!
                   !!neurone artificiel!!

      cf neuroneformel-1.png

      Version simplifié du neurone bio

      Les entrées
         noté X
         Représentent les dentrites
      Les sorties
         noté Y
         représentent l'axome
      paramètre noté w et b
         influencent le comportement du neurone

      !!Équation d'un neurone formel :!!
         !!y^=f(⟨w,x⟩+b)!!
         chaque entrée est multipliée par w (coef) puis sommées puis additionnées par un bias b. Puis le tout passe dans ue fonction de transfert (svt nn linéaire)
         c'est une !!modulation d'amplitude!! et non de fréquence comme dans neurone bio

   Apprentissage par descente de gradient
      c'est chercher w et b
      c'est l'optimisation d'une fonction de perte
         on cherche w et b qui minimise cette fonction

      Dans le cas de la classification la fonction de perte est
         L(y^,y)=−(y.log(y^)+(1−y).log(1−y^))
         Elle s'appelle log-vraisemblance négative
         Quand tout est dérivable, on peut utiliser la descente de gradient
            à chaque étape les paramètres sont déplacés de la manière suivante
               ∂L/∂wi=(∂L/∂y)*f′(⟨w,x⟩+b)xi
               cf linearsep_anim.gif

   Si tu as le temps
      https://fr.wikipedia.org/wiki/Perceptron

   SumUp
      neurone artificiel est inspiré neurone bio
      Dans un model linéarement dérivable et de données linéarement séparable nous avons vu comment apprendre w et b via descente de gradient


2. Explorez les réseaux de neurones en couches

   Limitation du neurone formel

      1 seul neurone ne peut pas résoudre pb complexe
         exemple : lorque les dataset ne puevent pas être séparées par une droite


   Mettre plusieurs neurones en réseau

     réseaux particuliers organisés en couches
        réseau multicouche
           cf reseau_multicouche.png
           le flux va tjs de Entrées vers Sorties
           Adapté pour data de taille fixe (comme image)
           Appelé : perceptron multicouche (PMC),
                    Feed-Forward
                    Multi Layer Perceptron (MLP)
        Réseau récurent
           cf resau_reccurent_rnn.png
           Apprentissage complex, on le verra plus tard
           Adaptée pour data de taille variable

   Organisation en une couche
      cf unecoucheisolée-1.png
      Sj=∑Wji*xi
      yj=f(Sj)
         où Wji représente la pondération entre l'entrée i et la sortie j.
         On a remplacé le biais b à l'aide d'une fausse entrée qui reste constante à la valeur 1.
             Ainsi,  Wj0 correspond au biais bj du neurone j.

      !!W forme une matrice contenant les paramètres de la couche!!

   Comment apprendre un réseau à une seule couche ?
      Descente de gradient
         ∂L/∂wj=(∂L/∂yj)*(∂yj/∂Sj)*(∂Sj/∂wji)
         ∂L/∂wj=(∂L/∂yj)*f′(Sj)xi

   Organisation en plusieurs couches
      On connecte la sortie de la couche n à l'entrée de la couche n+1

      S(l)=∑W(l)ji*I(l)i
      O(l)j=f(l)*(S(l)j)→I(l+1)
         Avec O la sortie
              I l'entrée
              (l) le numéro de la couche

   Rétropropagation du gradient
      Pour l'apprentissage du réseau
      Comment faire
         calculer le gradient des parametres de la derniere couche
         puis porpager jusqu'à la premiere de proche en proche

   Si tu as le temps
      https://fr.wikipedia.org/wiki/R%C3%A9seau_de_neurones_artificiels
      https://web.stanford.edu/~hastie/Papers/ESLII.pdf

   SumUp
      un neurone isolé ne peut pas résoudre de problème linéairement séparable.
      Il faut pour cela regrouper plusieurs neurones entre eux dans un réseau de neurones artificiels.
      Nous avons différents types de réseaux possibles
      les réseaux de neurones en couches dits perceptrons multicouches.
          apprentissage par un algorithme particulier, la rétropropagation du gradient

3. Initiez-vous aux autoencodeurs
   Architecture diabolo
      cf reseau_diabolo.png

      Compression/décompression de données
         encodeur
            prend les données en grande dimension et les compresse vers une plus petite dimension
         decodeur
            prend les données en petite dimension et les rétroprojette vers la plus grande dimension
         La valeur centrale est appelé !!h!!
         auto encodeur (AE)
            réseau dont la cible est l'entrée elle-même

      Apprentissage autoencoder
         rétropropagation du gradient

      Under/over complete
         2 types d'AE
            under complete
               nb unités centrales < nb unités d'entrée
               Pour réduction de dimension
            over-complete
               nb unités centrales > nb unités d'entrée
               ils cherchent une meilleure représentation des données pour un traitement ultérieur
                  correspond à la projection dans une plus grande dimension des SVM
                     SVM = Machine a vecteur de support ou Support vector machine
         chacun à ses particularités d'apprentissage

      Débruitage avec les autoencodeurs (under-complete)
         Améliorer apprentissage des under complete en rajoutant perturbation à l'entrée

      Prévenir la coadaptation sur les autoencodeurs (over-complete)
         Danger de tomber sur des solutions trivila
            recopier l'info de couche en couche
         Pour éviter cela, on deco ALEATOIREMENT des neurones de la représentation intermédiaire

       --> C'est du deep learning


   Réseaux profonds (données brutes vs caractéristiques)
      La phase d'extraction et classification des caractéristiques est dynamique
      On finit l'apprentissage de toutes les couches (y compris la première) par un apprentissage supervisé classique appelé !!fine-tuning!!.

   Problématique du gradient évanescent liée aux réseaux profonds
      Plus on rajoute de couche, plus apprentissage par retropropagation du gradient est difficile
      Focntion de transfert de tanh est très faible <-4 et >4
          On appelle ca le gradient vanishing (ou grandient evanescent)

   SumUp
      forme spéciale de réseau de neurones en couches
         autoencodeur, ou réseau diabolo
            adaptée à une tâche d'apprentissage non supervisé
               réduction de dimension
            effectue extraction de caractéristiques

         extraction de caractéristiques utile pour apprentissage profond

         apprentissage profond
            le NN attaque data brute
            Mais apprentissage difficil car vanishing gradient

